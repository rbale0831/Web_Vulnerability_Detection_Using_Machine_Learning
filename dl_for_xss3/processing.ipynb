{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87afd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import GeneSeg\n",
    "import csv,pickle,random,json\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold,KFold\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f2ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import precision_score,recall_score,accuracy_score,f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898b3b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dir=\"file\\\\word2vec.pickle\"\n",
    "pre_datas_trains=[\"file\\\\pre_datas_train1.csv\",\"file\\\\pre_datas_train2.csv\",\"file\\\\pre_datas_train3.csv\",\n",
    "                 \"file\\\\pre_datas_train4.csv\",\"file\\\\pre_datas_train5.csv\",\"file\\\\pre_datas_train6.csv\",\n",
    "                 \"file\\\\pre_datas_train7.csv\",\"file\\\\pre_datas_train8.csv\",\"file\\\\pre_datas_train9.csv\",\n",
    "                 \"file\\\\pre_datas_train10.csv\"]\n",
    "pre_datas_tests=[\"file\\\\pre_datas_test1.csv\",\"file\\\\pre_datas_test2.csv\",\"file\\\\pre_datas_test3.csv\",\n",
    "                \"file\\\\pre_datas_test4.csv\",\"file\\\\pre_datas_test5.csv\",\"file\\\\pre_datas_test6.csv\",\n",
    "                \"file\\\\pre_datas_test7.csv\",\"file\\\\pre_datas_test8.csv\",\"file\\\\pre_datas_test9.csv\",\n",
    "                \"file\\\\pre_datas_test10.csv\"]\n",
    "# pre_datas_validation=\"file\\\\pre_datas_validation.csv\"\n",
    "# process_datas_dir=\"file\\\\process_datas.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c2aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process():\n",
    "    with open(vec_dir,\"rb\") as f :\n",
    "        word2vec=pickle.load(f)\n",
    "        dictionary=word2vec[\"dictionary\"]\n",
    "        reverse_dictionary=word2vec[\"reverse_dictionary\"]\n",
    "        embeddings=word2vec[\"embeddings\"]\n",
    "    xssed_data=[]\n",
    "    normal_data=[]\n",
    "    with open(\"data\\\\xssed.csv\",\"r\",encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f, fieldnames=[\"payload\"])\n",
    "        for row in reader:\n",
    "            payload=row[\"payload\"]\n",
    "            word=GeneSeg(payload)\n",
    "            xssed_data.append(word)\n",
    "    with open(\"data\\\\normal_examples.csv\",\"r\",encoding=\"utf-8\") as f:\n",
    "        reader=csv.reader(f)\n",
    "        reader = csv.DictReader(f, fieldnames=[\"payload\"])\n",
    "        for row in reader:\n",
    "            payload=row[\"payload\"]\n",
    "            word=GeneSeg(payload)\n",
    "            normal_data.append(word)\n",
    "    xssed_num=len(xssed_data)\n",
    "    normal_num=len(normal_data)\n",
    "    xssed_labels=[1]*xssed_num\n",
    "    normal_labels=[0]*normal_num\n",
    "    datas=xssed_data+normal_data\n",
    "    labels=xssed_labels+normal_labels\n",
    "    labels=to_categorical(labels)\n",
    "    def to_index(data):\n",
    "        d_index=[]\n",
    "        for word in data:\n",
    "            if word in dictionary.keys():\n",
    "                d_index.append(dictionary[word])\n",
    "            else:\n",
    "                d_index.append(dictionary[\"UNK\"])\n",
    "        return d_index\n",
    "    datas_index=[to_index(data) for data in datas]\n",
    "    datas_index=pad_sequences(datas_index,value=-1)\n",
    "    rand=random.sample(range(len(datas_index)),len(datas_index))\n",
    "    datas=[datas_index[index] for index in rand]\n",
    "    labels=[labels[index] for index in rand]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    folder=KFold(n_splits=10,shuffle=False,random_state=0)\n",
    "    number=0\n",
    "    for train,test in folder.split(datas,labels):\n",
    "\n",
    "        train_datas=[datas[i] for i in train]\n",
    "        test_datas=[datas[i] for i in test]\n",
    "        train_labels=[labels[i] for i in train]\n",
    "        test_labels=[labels[i] for i in test]\n",
    "        train_size=len(train_labels)\n",
    "        test_size=len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdbfeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "        input_num=len(train_datas[0])\n",
    "        dims_num = embeddings[\"UNK\"].shape[0]\n",
    "        word2vec[\"train_size\"]=train_size\n",
    "        word2vec[\"test_size\"]=test_size\n",
    "        word2vec[\"input_num\"]=input_num\n",
    "        word2vec[\"dims_num\"]=dims_num\n",
    "\n",
    "        # print(train_size)\n",
    "        # print(test_size)\n",
    "        # print(input_num)\n",
    "        # print(dims_num)\n",
    "\n",
    "        print(\"Write trian datas\")\n",
    "        with open(pre_datas_trains[number],\"w\") as f:\n",
    "            for i in range(train_size):\n",
    "                data_line=str(train_datas[i].tolist())+\"|\"+str(train_labels[i].tolist())+\"\\n\"\n",
    "                f.write(data_line)\n",
    "\n",
    "        print(\"Write test datas\")\n",
    "        with open(pre_datas_tests[number],\"w\") as f:\n",
    "            for i in range(test_size):\n",
    "                data_line=str(test_datas[i].tolist())+\"|\"+str(test_labels[i].tolist())+\"\\n\"\n",
    "                f.write(data_line)\n",
    "\n",
    "        number=number+1\n",
    "\n",
    "    with open(vec_dir, \"wb\") as f:\n",
    "        pickle.dump(word2vec, f)\n",
    "    print(\"Saved word2vec to:\", vec_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87775164",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"Write datas over!\")\n",
    "def data_generator(data_dir):\n",
    "    reader = tf.TextLineReader()\n",
    "    queue = tf.train.string_input_producer([data_dir])\n",
    "    _, value = reader.read(queue)\n",
    "    coord = tf.train.Coordinator()\n",
    "    sess = tf.Session()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    while True:\n",
    "        v = sess.run(value)\n",
    "        [data, label] = v.split(b\"|\")\n",
    "        data = np.array(json.loads(data.decode(\"utf-8\")))\n",
    "        label = np.array(json.loads(label.decode(\"utf-8\")))\n",
    "        yield (data, label)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()\n",
    "def batch_generator(datas_dir,datas_size,batch_size,embeddings,reverse_dictionary,train=True):\n",
    "    batch_data = []\n",
    "    batch_label = []\n",
    "    generator=data_generator(datas_dir)\n",
    "    n=0\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            data,label=next(generator)\n",
    "            data_embed = []\n",
    "            for d in data:\n",
    "                if d != -1:\n",
    "                    data_embed.append(embeddings[reverse_dictionary[d]])\n",
    "                else:\n",
    "                    data_embed.append([0.0] * len(embeddings[\"UNK\"]))\n",
    "            batch_data.append(data_embed)\n",
    "            batch_label.append(label)\n",
    "            n+=1\n",
    "            if not train and n==datas_size:\n",
    "                break\n",
    "        if not train and n == datas_size:\n",
    "            yield (np.array(batch_data), np.array(batch_label))\n",
    "            break\n",
    "        else:\n",
    "            yield (np.array(batch_data),np.array(batch_label))\n",
    "            batch_data = []\n",
    "            batch_label = []\n",
    "def build_dataset(batch_size):\n",
    "    with open(vec_dir, \"rb\") as f:\n",
    "        word2vec = pickle.load(f)\n",
    "    embeddings = word2vec[\"embeddings\"]\n",
    "    reverse_dictionary = word2vec[\"reverse_dictionary\"]\n",
    "    train_size=word2vec[\"train_size\"]\n",
    "    test_size=word2vec[\"test_size\"]\n",
    "    dims_num = word2vec[\"dims_num\"]\n",
    "    input_num =word2vec[\"input_num\"]\n",
    "\n",
    "    train_generators=[]\n",
    "    test_generators=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88250765",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for i in range(10):\n",
    "        train_generator = batch_generator(pre_datas_trains[i],train_size,batch_size,embeddings,reverse_dictionary)\n",
    "        test_generator = batch_generator(pre_datas_tests[i],test_size,batch_size,embeddings,reverse_dictionary,train=False)\n",
    "        train_generators.append(train_generator)\n",
    "        test_generators.append(test_generator)\n",
    "\n",
    "    return train_generators,test_generators,train_size,test_size,input_num,dims_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80583467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC for a binary classifier\n",
    "def auc(y_true, y_pred):\n",
    "    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)\n",
    "    binSizes = -(pfas[1:]-pfas[:-1])\n",
    "    s = ptas*binSizes\n",
    "    return K.sum(s, axis=0)\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# PFA, prob false alert for binary classifier\n",
    "def binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # N = total number of negative labels\n",
    "    N = K.sum(1 - y_true)\n",
    "    # FP = total number of false alerts, alerts from the negative class labels\n",
    "    FP = K.sum(y_pred - y_pred * y_true)\n",
    "    return FP/N\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# P_TA prob true alerts for binary classifier\n",
    "def binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # P = total number of positive labels\n",
    "    P = K.sum(y_true)\n",
    "    # TP = total number of correct alerts, alerts from the positive class labels\n",
    "    TP = K.sum(y_pred * y_true)\n",
    "    return TP/P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cdd92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataTest(model_dir,test_generator,test_size,input_num,dims_num,batch_size):\n",
    "    model=load_model(model_dir,custom_objects={'auc':auc,'binary_PFA':binary_PFA,'binary_PTA':binary_PTA})\n",
    "    labels_pre=[]\n",
    "    labels_true=[]\n",
    "    batch_num=test_size//batch_size+1\n",
    "    steps=0\n",
    "    for batch,labels in test_generator:\n",
    "        if len(labels)==batch_size:\n",
    "            labels_pre.extend(model.predict_on_batch(batch))\n",
    "        else:\n",
    "            batch=np.concatenate((batch,np.zeros((batch_size-len(labels),input_num,dims_num))))\n",
    "            labels_pre.extend(model.predict_on_batch(batch)[0:len(labels)])\n",
    "        labels_true.extend(labels)\n",
    "        steps+=1\n",
    "        print(\"%d/%d batch\"%(steps,batch_num))\n",
    "    labels_pre=np.array(labels_pre).round()\n",
    "    def to_y(labels):\n",
    "        y=[]\n",
    "        for i in range(len(labels)):\n",
    "            if labels[i][0]==1:\n",
    "                y.append(0)\n",
    "            else:\n",
    "                y.append(1)\n",
    "        return y\n",
    "    y_true=to_y(labels_true)\n",
    "    y_pre=to_y(labels_pre)\n",
    "    precision=precision_score(y_true,y_pre)\n",
    "    recall=recall_score(y_true,y_pre)\n",
    "    accuracy=accuracy_score(y_true,y_pre)\n",
    "    f1=f1_score(y_true,y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c023f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"Precision score is :\",precision)\n",
    "    print(\"Recall score is :\",recall)\n",
    "    print(\"Accuracy score is : \",accuracy)\n",
    "    print(\"F1 score is : \",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pre)\n",
    "    roc_auc = ac(fpr, tpr)\n",
    "    print(\"FP rate is :\",fpr)\n",
    "    print(\"TP rate is :\",tpr)\n",
    "    print(\"Roc_AOC is :\",roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b691df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.switch_backend('agg')\n",
    "    # plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "    plt.plot(fpr, tpr, alpha=0.8)#, label='3C-LSTM(area=%0.4f)' % (roc_auc)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve for test dataset Zoom in at top left ')\n",
    "    # plt.legend(loc=\"lower right\")\n",
    "    plt.xlim(0,0.1)\n",
    "    plt.ylim(0.2,1)\n",
    "    plt.savefig('roc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    pre_process()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
